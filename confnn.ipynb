{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 100\n",
    "EVAL_BATCH_SIZE = 1\n",
    "EVAL_FREQUENCY = 5000  # Number of steps between evaluations.\n",
    "\n",
    "EMBEDDING_SIZE = 8\n",
    "NUM_UNROLL_STEPS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(filename):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not tf.gfile.Exists(WORK_DIRECTORY):\n",
    "    tf.gfile.MakeDirs(WORK_DIRECTORY)\n",
    "  filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "  if not tf.gfile.Exists(filepath):\n",
    "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    with tf.gfile.GFile(filepath) as f:\n",
    "      size = f.size()\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n",
    "  return labels\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "  argmax = numpy.argmax(predictions, 1)\n",
    "  print(Counter(argmax).most_common())\n",
    "  return 100.0 - (\n",
    "      100.0 *\n",
    "      numpy.sum(argmax == labels) /\n",
    "      predictions.shape[0])\n",
    "\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# Generate a validation set.\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_step(input_image, prior, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    assert input_image.get_shape()[0] == 1, \"HyperNet operates on single images only\"\n",
    "    \n",
    "    prior_embeddings = tf.get_variable(\"prior_embeddings\",\n",
    "                                        shape=[NUM_LABELS, EMBEDDING_SIZE],\n",
    "                                        initializer=tf.random_uniform_initializer(\n",
    "                                            minval=-1.0/numpy.sqrt(NUM_LABELS), maxval=1.0/numpy.sqrt(NUM_LABELS)))\n",
    "    embedding_features = tf.matmul(prior, prior_embeddings)\n",
    "    for i in range(4):\n",
    "        embedding_features = embedding_features + tf.layers.dense(\n",
    "            embedding_features, EMBEDDING_SIZE, use_bias=False, activation=tf.nn.relu)\n",
    "    \n",
    "    def get_dynamic_weights(weights_shape):\n",
    "        num_weights = numpy.prod(weights_shape)\n",
    "        dynamic_weights_flat = tf.layers.dense(embedding_features, num_weights)\n",
    "        dynamic_weights = tf.reshape(dynamic_weights_flat, weights_shape)\n",
    "        dynamic_weights.set_shape(weights_shape)\n",
    "        return dynamic_weights\n",
    "    \n",
    "    conv1_weights = get_dynamic_weights([5, 5, NUM_CHANNELS, 4])\n",
    "    conv1_biases = get_dynamic_weights([4])\n",
    "    conv2_weights = get_dynamic_weights([5, 5, 4, 8])\n",
    "    conv2_biases = get_dynamic_weights([8])\n",
    "    fc2_weights = get_dynamic_weights([8, NUM_LABELS])\n",
    "    fc2_biases = get_dynamic_weights([NUM_LABELS])\n",
    "    \n",
    "    conv = tf.nn.conv2d(input_image,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    pool_shape = pool.get_shape()\n",
    "    hidden = tf.nn.avg_pool(pool, ksize=[1, pool_shape[1], pool_shape[2], 1],\n",
    "                            strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "    hidden = tf.squeeze(hidden, axis=[1, 2])\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    #if train:\n",
    "    #    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    logits = tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "    posteriors = tf.nn.softmax(logits)\n",
    "    return logits, posteriors\n",
    "\n",
    "def apply(input_image, train):\n",
    "    priors = numpy.array([[1/NUM_LABELS for _ in range(NUM_LABELS)]], dtype=numpy.float32)\n",
    "    results = []\n",
    "    loss = 0.0\n",
    "    for step in range(NUM_UNROLL_STEPS):\n",
    "        logits, posteriors = model_step(input_image, priors, train=train)\n",
    "        priors = posteriors\n",
    "        results.append((logits, posteriors))\n",
    "        loss += tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "              labels=train_labels_node, logits=logits))\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 0.4 ms\n",
      "Minibatch loss: 6.671, learning rate: 0.001000\n",
      "[(1, 1)]\n",
      "Minibatch error: 100.0%\n",
      "[(1, 5000)]\n",
      "Validation error: 88.7%\n",
      "Step 5000 (epoch 0.09), 9.4 ms\n",
      "Minibatch loss: 3.105, learning rate: 0.001000\n",
      "[(3, 1)]\n",
      "Minibatch error: 0.0%\n",
      "[(0, 736), (8, 606), (4, 543), (7, 537), (1, 527), (6, 471), (9, 469), (3, 413), (5, 395), (2, 303)]\n",
      "Validation error: 17.2%\n",
      "Step 10000 (epoch 0.18), 9.2 ms\n",
      "Minibatch loss: 0.448, learning rate: 0.001000\n",
      "[(5, 1)]\n",
      "Minibatch error: 0.0%\n",
      "[(6, 560), (4, 535), (1, 531), (8, 521), (9, 511), (2, 500), (7, 498), (5, 491), (0, 480), (3, 373)]\n",
      "Validation error: 10.7%\n",
      "Step 15000 (epoch 0.27), 9.5 ms\n",
      "Minibatch loss: 0.194, learning rate: 0.001000\n",
      "[(5, 1)]\n",
      "Minibatch error: 0.0%\n",
      "[(1, 589), (9, 550), (5, 535), (6, 534), (8, 491), (4, 489), (0, 478), (3, 462), (7, 458), (2, 414)]\n",
      "Validation error: 9.1%\n",
      "Step 20000 (epoch 0.36), 9.5 ms\n",
      "Minibatch loss: 1.548, learning rate: 0.001000\n",
      "[(3, 1)]\n",
      "Minibatch error: 0.0%\n",
      "[(7, 586), (1, 543), (4, 527), (6, 512), (8, 507), (2, 500), (0, 473), (3, 469), (5, 444), (9, 439)]\n",
      "Validation error: 5.8%\n",
      "Step 25000 (epoch 0.45), 9.1 ms\n",
      "Minibatch loss: 0.013, learning rate: 0.001000\n",
      "[(3, 1)]\n",
      "Minibatch error: 0.0%\n",
      "[(1, 562), (9, 552), (4, 545), (7, 536), (6, 510), (2, 500), (8, 477), (3, 469), (0, 458), (5, 391)]\n",
      "Validation error: 5.9%\n"
     ]
    }
   ],
   "source": [
    "train_data_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n",
    "eval_data = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0, dtype=tf.float32)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    1e-3,                # Base learning rate.\n",
    "    batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "    train_size,          # Decay step.\n",
    "    0.95,                # Decay rate.\n",
    "    staircase=True)\n",
    "\n",
    "# Predictions for the current training minibatch.\n",
    "with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
    "    train_prediction, loss = apply(train_data_node, train=True)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,\n",
    "                                                   global_step=batch)\n",
    "\n",
    "\n",
    "# Predictions for the test and validation, which we'll compute less often.\n",
    "with tf.variable_scope(\"model\", reuse=True):\n",
    "    eval_prediction, _ = apply(eval_data, train=False)\n",
    "\n",
    "  # Small utility function to evaluate a dataset by feeding batches of data to\n",
    "  # {eval_data} and pulling the results from {eval_predictions}.\n",
    "  # Saves memory and enables this to run on smaller GPUs.\n",
    "def eval_in_batches(data, sess):\n",
    "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "      end = begin + EVAL_BATCH_SIZE\n",
    "      if end <= size:\n",
    "        predictions[begin:end, :] = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[begin:end, ...]})\n",
    "      else:\n",
    "        batch_predictions = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n",
    "  # Create a local session to run the training.\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized!')\n",
    "    # Loop through training steps.\n",
    "    for step in xrange(int(NUM_EPOCHS * train_size) // BATCH_SIZE):\n",
    "      # Compute the offset of the current minibatch in the data.\n",
    "      # Note that we could use better randomization across epochs.\n",
    "      offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "      batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "      batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "      # This dictionary maps the batch data (as a numpy array) to the\n",
    "      # node in the graph it should be fed to.\n",
    "      feed_dict = {train_data_node: batch_data,\n",
    "                   train_labels_node: batch_labels}\n",
    "      # Run the optimizer to update weights.\n",
    "      sess.run(optimizer, feed_dict=feed_dict)\n",
    "      # print some extra information once reach the evaluation frequency\n",
    "      if step % EVAL_FREQUENCY == 0:\n",
    "        # fetch some extra nodes' data\n",
    "        l, lr, predictions = sess.run([loss, learning_rate, train_prediction],\n",
    "                                      feed_dict=feed_dict)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        print('Step %d (epoch %.2f), %.1f ms' %\n",
    "              (step, float(step) * BATCH_SIZE / train_size,\n",
    "               1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "        print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "            eval_in_batches(validation_data, sess), validation_labels))\n",
    "        sys.stdout.flush()\n",
    "    # Finally print the result!\n",
    "    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
